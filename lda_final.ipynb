{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.decomposition._online_lda import _dirichlet_expectation_2d\n",
    "\n",
    "import utils as u\n",
    "\n",
    "from lda_directed import DirectedLDA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../pandas/lemma_delivered_merged_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_nn'] = pd.read_pickle('../pandas/is_nn_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_corpus = df[(df['agg_label'] != -1) & (df['is_nn'] == False)]\n",
    "unlabeled_corpus = df[(df['agg_label'] == -1) & (df['is_nn'] == False)]\n",
    "\n",
    "target = 'agg_label'\n",
    "text = 'lemma_delivered'\n",
    "fasttext = 'ft'\n",
    "numeric = ['raw_len', 'raw_word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, vali_X, train_y, vali_y = train_test_split(\n",
    "    labeled_corpus,\n",
    "    labeled_corpus[target],\n",
    "    test_size=0.4,\n",
    "    random_state=1,\n",
    "    stratify=labeled_corpus[target])\n",
    "\n",
    "test_X, validation_X, test_y, validation_y = train_test_split(\n",
    "    vali_X,\n",
    "    vali_y,\n",
    "    test_size=0.5,\n",
    "    random_state=1,\n",
    "    stratify=vali_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_score_vocab(train_X, train_y, n_top_words=5400):\n",
    "\n",
    "    vec = CountVectorizer(**{'lowercase': False, 'max_df': 0.25})\n",
    "    vec_train_X = vec.fit_transform(train_X)\n",
    "\n",
    "    score, _ = chi2(vec_train_X, train_y)\n",
    "\n",
    "    vocab = vec.get_feature_names()\n",
    "\n",
    "    result = [vocab[i] for i in score.argsort()[:-n_top_words - 1:-1]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_vocab = direct_score_vocab(train_X[text], train_y, n_top_words=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(direct_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vocabs = u.get_vocabs(train_X[text], train_y, CountVectorizer(**{'lowercase': False, 'max_df': 0.25}))\n",
    "merged_vocabs = u.merge_vocabs_by_score(class_vocabs, train_X[text], train_y, \n",
    "                                      CountVectorizer, {'lowercase': False, 'max_df': 0.25}, \n",
    "                                      chi2, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17415"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6590207914151576"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer(**{'lowercase': False, 'max_df': 0.25, 'vocabulary': direct_vocab})\n",
    "\n",
    "vec_train_X = vec.fit_transform(train_X[text])\n",
    "vec_validation_X = vec.transform(validation_X[text])\n",
    "\n",
    "clf = LinearSVC(**{'C': 0.5, 'loss': 'squared_hinge', 'tol': 0.01})\n",
    "clf.fit(vec_train_X, train_y)\n",
    "\n",
    "clf_preds = clf.predict(vec_validation_X)\n",
    "np.mean(clf_preds == validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_preds_probs(sample, train_X, train_y):\n",
    "\n",
    "    vec = CountVectorizer(**{'lowercase': False, 'vocabulary': direct_vocab})\n",
    "\n",
    "    vec_train_X = vec.fit_transform(train_X)\n",
    "    vec_sample_X = vec.transform(sample)\n",
    "    \n",
    "    priors = u.get_priors(vec_train_X, train_y, chi2)\n",
    "    \n",
    "    num_classes = len(set(train_y))\n",
    "    \n",
    "    lda = DirectedLDA(n_components=num_classes, max_iter=2,\n",
    "        learning_method='online',\n",
    "        learning_offset=50.,\n",
    "        random_state=0,\n",
    "        n_jobs=-1,\n",
    "        ptws=priors)\n",
    "    \n",
    "    lda.fit(vec_train_X)\n",
    "    \n",
    "    topic_train_probs = lda.transform(vec_train_X)\n",
    "    topic_probs = lda.transform(vec_sample_X)\n",
    "    topic_preds = [priors[x][0] for x in np.argmax(topic_probs, axis=1)]\n",
    "    \n",
    "    return topic_train_probs, topic_probs, topic_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02749832327297116"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_train_probs, topic_probs, topic_preds = get_topic_preds_probs(validation_X[text], train_X[text], train_y)\n",
    "\n",
    "np.mean(topic_preds == validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DirectedLDA(batch_size=128, doc_topic_prior=None, evaluate_every=-1,\n",
       "            learning_decay=0.7, learning_method='online', learning_offset=50.0,\n",
       "            max_doc_update_iter=100, max_iter=2, mean_change_tol=0.001,\n",
       "            n_components=27, n_jobs=-1, perp_tol=0.1,\n",
       "            ptws=[(1,\n",
       "                   array([0.14162873, 0.19828022, 0.62316641, ..., 0.05665149, 1.71405569,\n",
       "       0.08497724])),\n",
       "                  (2,\n",
       "                   array([0.09245343, 0.1294348 , 0.4...\n",
       "       1.40894569e+02, 4.25850340e-01, 4.25850340e-02])),\n",
       "                  (91,\n",
       "                   array([0.41476998, 0.43445976, 1.82498789, ..., 0.16590799, 2.48861985,\n",
       "       0.24886199])),\n",
       "                  (92,\n",
       "                   array([2.30132373e-01, 3.22185322e-01, 2.12761189e+02, ...,\n",
       "       9.20529492e-02, 1.38079424e+00, 1.38079424e-01])),\n",
       "                  (93,\n",
       "                   array([0.45785132, 0.64099185, 2.01454581, ..., 0.18314053, 2.74710792,\n",
       "       0.27471079]))],\n",
       "            random_state=0, topic_word_prior=None, total_samples=1000000.0,\n",
       "            verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(**{'lowercase': False, 'vocabulary': merged_vocabs})\n",
    "#vec = CountVectorizer(**{'lowercase': False, 'max_df': 0.25})\n",
    "\n",
    "vec_train_X = vec.fit_transform(train_X[text])\n",
    "vec_sample_X = vec.transform(validation_X[text])\n",
    "\n",
    "priors = u.get_priors(vec_train_X, train_y, chi2)\n",
    "\n",
    "lda = DirectedLDA(n_components=len(set(train_y)), max_iter=2,\n",
    "        learning_method='online',\n",
    "        learning_offset=50.,\n",
    "        random_state=0,\n",
    "        n_jobs=-1,\n",
    "        ptws=priors)\n",
    "    \n",
    "lda.fit(vec_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: prosent krone øke enn milliard lav høy 000 penge skatt tall økonomi bank vekst inntekt million økonomisk betale fjor all rente stat neste redusere falle\n",
      "Topic #1: kvinne mann samfunn mot muslim debatt all homofil lov muslimsk jøde demokrati rettighet forbud islam kultur ytringsfrihet politisk religiøs holdning frihet vold menneske menneskerettighet abort\n",
      "Topic #2: sykehus ved lege pasient behandling sykdom enn kvinne medisin syk undersøkelse psykisk kropp medisinsk helse forsker alvorlig all risiko hos vise øke dø Ullevål forskning\n",
      "Topic #3: mat spise hund dyr produkt fisk all enn bonde kilo kjøtt fiske katt laks frukt Mattilsynet sunn inneholde butikk forbruker Tine gård melk vann matvare\n",
      "Topic #4: ansatt jobb bedrift jobbe offentlig LO lønn arbeid pensjon ordning leder privat stilling arbeidsliv NHO pensjonist Nav streik enn sektor arbeidsplass ledelse direktør virksomhet medarbeider\n",
      "Topic #5: skole elev ved universitet fag videregående utdanning professor lære klasse all forskning enn studium kunnskap Universitetet Oslo karakter utdannelse undervisning forsker rektor resultat høyskole høy\n",
      "Topic #6: utslipp CO2 klima forsker miljø verden art global dyr grønn natur ulv tonn CO søppel Solheim område del energi plast hav skyte jakt miljøvennlig redusere\n",
      "Topic #7: olje selskap gass pris strøm Hydro Norge oljepris enn produksjon fat oljeselskap energi Svalbard norsk felt Lofoten øre del sokkel Barentshavet Henriksen dollar all bensin\n",
      "Topic #8: Norge land innvandrer norsk opphold søke reise søknad person UDI sende sak Pakistan avslag bo myndighet hjemland pakistansk asylmottak pass Somalia asyl gruppe Hanssen innvandring\n",
      "Topic #9: bil fly SAS passasjer all tog Oslo ulykke buss reise trafikk enn km flyplass skip flyselskap bord sjåfør NSB tunnel Gardermoen bilist sykkel pilot rute\n",
      "Topic #10: politi sak mann åring mot hun dømme kvinne Oslo gammel ved går advokat drap ifølge mene tidlig hans sikte person dom drepe tiltale fortelle dette\n",
      "Topic #11: jeg hun barn min all enn vite gjøre selv familie liv ung når forelder hva god mor fortelle sammen mann eller føle bare synes gammel\n",
      "Topic #12: Oslo by hus hytte bygge bo eiendom leilighet kjøpe selge flytte krone kvadratmeter 000 pris arkitekt bydel prosjekt område del leie kommune bygg bygning enn\n",
      "Topic #13: krone million selskap prosent selge kjøpe aksje pris milliard penge betale kunde enn all går marked fjor salg butikk eie bank eier del Røkke direktør\n",
      "Topic #14: USA mot krig amerikansk Irak drepe soldat president Israel angrep militær FN Afghanistan land all styrke går Bush by ifølge NATO amerikaner al israelsk Forsvaret\n",
      "Topic #15: TV NRK medium nett avis Telenor program journalist kanal mobil enn Facebook digital internett PC telefon bilde sende mobiltelefon nyhet tjeneste Internett TV2 radio BBC\n",
      "Topic #16: svensk alkohol øl vin lite vare Singapore tobakk WTO Fredriksen spritt Vinmonopolet container Tollvesenet avgift toller Venezuela grense import sigarett smugling brennevin billig Svinesund smugle\n",
      "Topic #17: land EU Europa USA Russland Norge verden Kina russisk Tyskland internasjonal president europeisk Frankrike politisk tysk avtale myndighet norsk Storbritannia ny økonomisk enn mot Putin\n",
      "Topic #18: parti regjering Ap politisk Frp Høyre Stortinget valg leder statsminister Stoltenberg politiker Regjeringen enn politikk stemme SV sak velger Jens all hun Hagen forslag Arbeiderpartiet\n",
      "Topic #19: kommune område natur ordfører skog vern Drøbak fylke fjell samisk grunneier dispensasjon terreng verne nasjonalpark funn fylkeskommune Fylkesmannen same frede tilskudd areal erstatning arkeolog kulturminne\n",
      "Topic #20: norsk Oslo museum kunst Norsk all Munch ny kultur Trond utstilling kunstner Giske festival forlag opera del kulturminister teater bok maleri stiftelse direktør Bjørvika publikum\n",
      "Topic #21: OL idrett Tromsø fotball VM klubb utøver Norges NFF trener internasjonal IOC spiller doping forbund komité ski generalsekretær trene president styre Lillehammer Olympiatoppen arrangere FIFA\n",
      "Topic #22: går ved skade NTB kveld melde natt båt ulykke meter person vann område omkomme brann dø klokke menneske død ettermiddag gammel redde liv opplyse mann\n",
      "Topic #23: Gud biskop prest religion menneske religiøs pave menighet Jesus katolsk hellig Den Kirken medlem Bibelen kristendom Johannes jul moské all Gelius muslim Teresa Jerusalem Kirkens\n",
      "Topic #24: bok hans historie bilde film jeg hun enn The forfatter selv navn liv verden spille lese fortelle lage musikk avis rolle vite serie her of\n",
      "Topic #25: 30 10 00 15 20 12 18 11 13 kamp 16 17 14 19 22 25 23 lag 21 40 spille spiller 24 mål 45\n",
      "Topic #26: dette du god gjøre ny eller man norsk Norge når gi ved slik liten mene bare selv bruke vise mot vår hva viktig her mellom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = vec.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_train_probs = lda.transform(vec_train_X)\n",
    "topic_probs = lda.transform(vec_sample_X)\n",
    "topic_preds = [priors[x][0] for x in np.argmax(topic_probs, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3761234071093226"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(topic_preds == validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(**{'lowercase': False, 'max_df': 0.25})\n",
    "\n",
    "vec_train_X = vec.fit_transform(train_X[text])\n",
    "vec_validation_X = vec.transform(validation_X[text])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_topic_train_probs = scaler.fit_transform(topic_train_probs)\n",
    "scaled_topic_probs = scaler.fit_transform(topic_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_X = scipy.sparse.hstack([vec_train_X, scaled_topic_train_probs])\n",
    "merged_validation_X = scipy.sparse.hstack([vec_validation_X, scaled_topic_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67981220657277"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(**{'C': 0.5, 'loss': 'squared_hinge', 'tol': 0.01})\n",
    "clf.fit(vec_train_X, train_y)\n",
    "\n",
    "clf_preds = clf.predict(vec_validation_X)\n",
    "np.mean(clf_preds == validation_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

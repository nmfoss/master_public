{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils as u\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from gensim.models.fasttext import load_facebook_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nils\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "#fbkv = load_facebook_vectors('../../semester03/DAT620/datafiles/81/parameters.bin')\n",
    "fbkv = load_facebook_vectors('../datafiles/80/parameters.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle('../pandas/lemma_delivered_merged_df.pkl')\n",
    "df = pd.read_pickle('../pandas/lemma_delivered_merged_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_nn'] = pd.read_pickle('../pandas/is_nn_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_corpus = df[(df['agg_label'] != -1) & (df['is_nn'] == False)]\n",
    "unlabeled_corpus = df[(df['agg_label'] == -1) & (df['is_nn'] == False)]\n",
    "\n",
    "#labeled_corpus = df[(df['agg_label'] != -1)]\n",
    "#labeled_corpus = df[(df['agg_label'] != -1)]['lemma_delivered']\n",
    "#target = df[(df['agg_label'] != -1) & (df['agg_label'] < 90)]['agg_label']\n",
    "#target = df[(df['agg_label'] != -1)]['agg_label']\n",
    "\n",
    "#unlabeled_corpus = df[(df['agg_label'] == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, vali_X, train_y, vali_y = train_test_split(\n",
    "    labeled_corpus,\n",
    "    labeled_corpus['agg_label'],\n",
    "    test_size=0.4,\n",
    "    random_state=1,\n",
    "    stratify=labeled_corpus['agg_label'])\n",
    "\n",
    "test_X, validation_X, test_y, validation_y = train_test_split(\n",
    "    vali_X,\n",
    "    vali_y,\n",
    "    test_size=0.5,\n",
    "    random_state=1,\n",
    "    stratify=vali_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_tfidf_weighted(ft, train, corpus):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**{'lowercase': False, 'max_df': 0.25})\n",
    "    vectorizer.fit(train)\n",
    "    vectorized = vectorizer.transform(corpus)\n",
    "    \n",
    "    embedding_matrix = np.zeros((vectorized.shape[1], 100))\n",
    "    \n",
    "    words = []\n",
    "    for index, word in enumerate(vectorizer.get_feature_names()):\n",
    "        words.append(word)\n",
    "        embedding_matrix[index] = ft.get_vector(word)\n",
    "\n",
    "    tfidf_weighted_vecs = []\n",
    "    for index in range(vectorized.shape[0]):\n",
    "        \n",
    "        doc_indices = vectorized.getrow(index).indices\n",
    "        doc_vec = embedding_matrix[doc_indices]\n",
    "        doc_weights = np.asarray(vectorized.getrow(index).todense()[0,doc_indices]).reshape(-1)\n",
    "        weighted = np.dot(doc_weights.T, doc_vec) / np.sum(doc_weights)\n",
    "        tfidf_weighted_vecs.append( weighted )\n",
    "        \n",
    "    return tfidf_weighted_vecs\n",
    "\n",
    "\n",
    "def get_fasttext_tfidf_weighted_chi2xidf(ft, t_X, t_y, train, corpus):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**{'lowercase': False, 'max_df': 0.25})\n",
    "    \n",
    "    vectorizer.fit(train)\n",
    "    \n",
    "    t_X_vec = vectorizer.transform(t_X)\n",
    "    \n",
    "    scores, pvals = chi2(t_X_vec, t_y)\n",
    "    \n",
    "    vectorizer.idf_ = vectorizer.idf_ * scores\n",
    "    \n",
    "    vectorized = vectorizer.transform(corpus)\n",
    "    \n",
    "    embedding_matrix = np.zeros((vectorized.shape[1], 100))\n",
    "    \n",
    "    words = []\n",
    "    for index, word in enumerate(vectorizer.get_feature_names()):\n",
    "        words.append(word)\n",
    "        embedding_matrix[index] = ft.get_vector(word)\n",
    "\n",
    "    tfidf_weighted_vecs = []\n",
    "    for index in range(vectorized.shape[0]):\n",
    "        \n",
    "        doc_indices = vectorized.getrow(index).indices\n",
    "        doc_vec = embedding_matrix[doc_indices]\n",
    "        doc_weights = np.asarray(vectorized.getrow(index).todense()[0,doc_indices]).reshape(-1)\n",
    "        weighted = np.dot(doc_weights.T, doc_vec) / np.sum(doc_weights)\n",
    "        tfidf_weighted_vecs.append( weighted )\n",
    "        \n",
    "    return tfidf_weighted_vecs\n",
    "\n",
    "\n",
    "def get_fasttext_tfidf_weighted_small_vocab(ft, vocab, corpus):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**{'lowercase': False, 'max_df': 0.25, 'tokenizer': None, 'vocabulary': vocab})\n",
    "    vectorized = vectorizer.fit_transform(corpus.values)\n",
    "    \n",
    "    embedding_matrix = np.zeros((vectorized.shape[1], 100))\n",
    "    \n",
    "    words = []\n",
    "    for index, word in enumerate(vectorizer.get_feature_names()):\n",
    "        words.append(word)\n",
    "        embedding_matrix[index] = ft.get_vector(word)\n",
    "\n",
    "    tfidf_weighted_vecs = []\n",
    "    for index in range(vectorized.shape[0]):\n",
    "        \n",
    "        doc_indices = vectorized.getrow(index).indices\n",
    "        doc_vec = embedding_matrix[doc_indices]\n",
    "        doc_weights = np.asarray(vectorized.getrow(index).todense()[0,doc_indices]).reshape(-1)\n",
    "        weighted = np.dot(doc_weights.T, doc_vec) / np.sum(doc_weights)\n",
    "        tfidf_weighted_vecs.append( weighted )\n",
    "        \n",
    "    return tfidf_weighted_vecs\n",
    "\n",
    "def FT_to_matrix(data):\n",
    "    M = np.zeros((len(data), 100))\n",
    "    for index in range(len(data)):\n",
    "        M[index] = data[index]\n",
    "        \n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FT_TFIDF_full_vocab'] = get_fasttext_tfidf_weighted(fbkv, df.loc[~df.index.isin(vali_X.index)]['lemma_delivered'], df['lemma_delivered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FT_TFIDF_lemma_labeled_vocab'] = get_fasttext_tfidf_weighted(fbkv, train_X['lemma_delivered'], df['lemma_delivered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FT_TFIDF_lemma_labeled_vocab'].to_pickle('../pandas/FT_TFIDF_lemma_labeled_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FT_TFIDF_lemma_chixidf'] = get_fasttext_tfidf_weighted_chi2xidf(fbkv, train_X['lemma_delivered'], train_y, df.loc[~df.index.isin(vali_X.index)]['lemma_delivered'], df['lemma_delivered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df.index.isin(vali_X.index)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../pandas/lemma_delivered_merged_FT_TFIDF_lemma_full_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vocabs = u.get_vocabs(train_X['lemma_delivered'], train_y, CountVectorizer(**{'tokenizer': None, 'lowercase': False, 'max_df': 0.25}))\n",
    "merged_vocabs = u.merge_vocabs_by_score(class_vocabs, train_X['lemma_delivered'], train_y, \n",
    "                                  CountVectorizer, {'tokenizer': None, 'lowercase': False, 'max_df': 0.25}, \n",
    "                                  chi2, 2000)\n",
    "\n",
    "vec = CountVectorizer(**{'lowercase': False, 'max_df': 0.25, 'vocabulary': merged_vocabs})\n",
    "vectorized = vec.fit_transform(df['lemma_delivered'])\n",
    "\n",
    "counter = 0\n",
    "\n",
    "indices = []\n",
    "\n",
    "for index in range(vectorized.shape[0]):\n",
    "    doc_indices = vectorized.getrow(index).indices\n",
    "    doc_weights = np.asarray(vectorized.getrow(index).todense()[0,doc_indices]).reshape(-1)\n",
    "    if np.sum(doc_weights) == 0:\n",
    "        counter += 1\n",
    "        indices.append(index)\n",
    "        \n",
    "print(counter)\n",
    "#print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(**{'lowercase': False, 'max_df': 0.25, 'vocabulary': merged_vocabs})\n",
    "vectorized = vec.fit_transform(df['lemma_delivered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "indices = []\n",
    "\n",
    "for index in range(vectorized.shape[0]):\n",
    "    doc_indices = vectorized.getrow(index).indices\n",
    "    doc_weights = np.asarray(vectorized.getrow(index).todense()[0,doc_indices]).reshape(-1)\n",
    "    if np.sum(doc_weights) == 0:\n",
    "        counter += 1\n",
    "        indices.append(index)\n",
    "        \n",
    "print(counter)\n",
    "#print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.drop(df.iloc[indices].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['FT_TFIDF_train_2000'] = get_fasttext_tfidf_weighted_small_vocab(fbkv, merged_vocabs, df_dropped['lemma_delivered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['FT_TFIDF_train_2000'].to_pickle('../pandas/FT_TFIDF_train_2000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(**{'lowercase': False, 'max_df': 0.25}) \n",
    "vec_train_X = vec.fit_transform(train_X['lemma_delivered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, pvals = chi2(vec_train_X, train_y)\n",
    "vec.idf_ = scores * vec.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_vali_X = vec.transform(vali_X['lemma_delivered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(**{'C': 7, 'class_weight': None, 'loss': 'hinge', 'tol': 0.0001, 'max_iter': 5000})\n",
    "model = CalibratedClassifierCV(svm)\n",
    "\n",
    "model.fit(vec_train_X, train_y)\n",
    "svm_preds = model.predict(vec_vali_X)\n",
    "\n",
    "np.mean(svm_preds == vali_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(**{'C': 7, 'class_weight': None, 'loss': 'hinge', 'tol': 0.0001, 'max_iter': 5000})\n",
    "model = CalibratedClassifierCV(svm)\n",
    "\n",
    "model.fit(np.stack(df_dropped.loc[df_dropped.index.isin(train_X.index)]['FT_TFIDF_train_2000'].to_numpy()), df_dropped.loc[df_dropped.index.isin(train_X.index)]['agg_label'])\n",
    "svm_preds = model.predict(np.stack(df_dropped.loc[df_dropped.index.isin(vali_X.index)]['FT_TFIDF_train_2000'].to_numpy()))\n",
    "\n",
    "np.mean(svm_preds == df_dropped.loc[df_dropped.index.isin(vali_X.index)]['agg_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(**{'C': 7, 'class_weight': None, 'loss': 'hinge', 'tol': 0.0001, 'max_iter': 5000})\n",
    "model = CalibratedClassifierCV(svm)\n",
    "\n",
    "model.fit(np.stack(train_X['FT_TFIDF_lemma_full_vocab'].to_numpy()), train_X['agg_label'])\n",
    "svm_preds = model.predict(np.stack(vali_X['FT_TFIDF_lemma_full_vocab'].to_numpy()))\n",
    "\n",
    "np.mean(svm_preds == vali_X['agg_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_dropped.index.isin(vali_X.index)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.loc[df_dropped.index.isin(vali_X.index)]['agg_label']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
